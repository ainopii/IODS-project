---
title: "chapter4.Rmd"
author: "Aino-Kaisa Piironen"
date: "2023-11-23"
output: html_document
---

# Clustering and classification

```{r}
date()
```

## About the data

```{r}
# Access the MASS package
library(MASS)

# and load the data
data("Boston")

# exploring the dataset
str(Boston)

# more information on the variables
?Boston

```

Boston data includes housing data in suburb of Boston. Data consists of 14 numerical variables and 506 observations.   
- **crim**: per capita crime rate by town.    
- **zn**: proportion of residential land zoned for lots over 25,000 sq.ft.    
- **indus**: proportion of non-retail business acres per town.  
- **chas**: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).  
- **nox**: nitrogen oxides concentration (parts per 10 million).  
- **rm**: average number of rooms per dwelling.  
- **age**: proportion of owner-occupied units built prior to 1940.  
- **dis**: weighted mean of distances to five Boston employment centres.  
- **rad**: index of accessibility to radial highways.  
- **tax**: full-value property-tax rate per $10,000.  
- **ptratio**: pupil-teacher ratio by town.  
- **black**: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.  
- **lstat**: lower status of the population (percent).  
- **medv**: median value of owner-occupied homes in $1000s.  

### Graphical overview of the data
```{r}
# plot matrix of the variables
p1 <- pairs(Boston)

library(tidyr)
library(corrplot)
#correlation plot
# calculate the correlation matrix and round it
cor_matrix <- cor(Boston) %>% round(digits = 2)

# visualize the correlation matrix
p2 <- corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)

# summaries of the variables
summary(Boston)
```
The plot matrix includes each pair of variables existing in the data. The plot matrix might not look very clear but it shows some overall distributions and possible relationships between variables. For example, there seem to be two clusters in the pairs including *chas*, *rad* and *tax* variables. 

The correlation plot shows the overview of the relationships between different variables. Blue color indicates here positive relationship and red negative relationship. The size of the circle and the intense of the color indicates the strength of the relationship. For example, the plot reveals the strongest positive relationships between *rad* and *tax* and the strongest negative relationships between *age* and *dis*, *lstat* and *medv*, *nox* and *dis*, and *indust* and *dis*. *Chas* variable shows the weakest or no relationship with any other variable.

The summary matrix shows that some variables, like *crim*, *zn*, and *black* seems to be skewed while some variables, like *nox*, *medv*, and *rm* are more normally distributed. Additionally, the values vary largely between variables from 0 to hundreds. So, observations of the variables seems to be in different scales.

## Data processing

### Standardizing the dataset
```{r}
# still working with
library(MASS)

# Standardizing the dataset
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# checking the class of the boston_scaled object
class(boston_scaled) # "matrix", "array"

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

# and change scaled crim as numeric
boston_scaled$crim <- as.numeric(boston_scaled$crim)

```

After standardizing the variables, we can see that all variables have now zero as a mean and observations are spreading around the zero. All variables are now in the same scale.

### Categorical variable of the scaled crime rate
```{r}
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

```

### Splitting the data to train and test sets
```{r}
library(dplyr)
library(MASS)

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set, including the randomly selecting rows (80 %)
train <- boston_scaled[ind,]

# create test set, including all the remaining 20 % of the rows.
test <- boston_scaled[-ind,]


```

Now we are ready for discriminant analysis...  

## Linear discriminant analysis

```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale =2.5)

```

The results of the LDA models shows that there are three linear discriminants in the model: LD1, LD2, and LD3. Each of these are a combination of variables that give the (three) best possible separation between groups. Proportion of trace is the percentage separation achieved by each discriminant function. Thus, the LD1 can separate different groups (or here crime classes) with over 90 % accuracy. To note, the separation percentages for LD2 and LD3 are low (only 1-5 %).

*Rad* shows the strongest (positive) relationship with the crime as the coefficient is around 3 in LD1. Additionally, *nox* and *age* seems to have positive relationship with the target *crim*.

To visually support this, the LDA plot shows the cluster of "high" crime group which is separating from the other classes in LD1 mainly due to the accessibility to radial highways (*rad*). There are also some med_high cases included in this cluster. The other cluster (for LD1) consists of three other groups: low, med_low and most med_high cases. Low group seems to separate from the med_high cases in LD2, mainly due to the *zn* (low), *nox* and *medv* (med_high) variables. Med_low cases seems to be between low and med_high cases.

### LDA prediction
```{r}
library(MASS)

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```
 
The cross-tabulation of predicted and correct classed shows that the LDA model is predicting correctly the high class and also quite nicely the low class. For the middle classes there are increasing number of incorrect predictions but still most of the cases are correctly predicted. Thus, the prediction performance of the LDA model is good and excellent for the high class.

### Distance measures
```{r}
library(MASS)
data("Boston") #reloading the original data

# standardizing the dataset
boston_scaled2 <- scale(Boston)

# calculating Euclidean distances and summarize 
dist_eu <- dist(boston_scaled2)
summary(dist_eu)

```
### K-means clustering
```{r}
library(MASS)
library(ggplot2)
set.seed(13)

# Run k-means algorithm on the dataset with 4 clusters
km <- kmeans(boston_scaled2, centers = 4)

# investigating the optimal number of clusters. MASS, scaled data already available.
set.seed(123)

# determine the max number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')


```

The plot above shows that the optimal number of clusters is 2 as the line is dramatically dropping at that point.

```{r}
library(MASS)
library(ggplot2)
set.seed(13)

# Run k-means algorithm on the dataset with 2 clusters
km <- kmeans(boston_scaled2, centers = 2)

# plot the Boston dataset with clusters
# pairs(boston_scaled2, col = km$cluster) for overall view, but it's not very clear
# needs to be data frame to plot only some columns, chosen based on the LDA plot.

pairs(as.data.frame(boston_scaled2)[c("crim", "rad", "nox", "medv", "zn")], col = km$cluster)

```

The plot shows that these two clusters are not perfectly separating from each others. Similar was also seen earlier in the LDA plot and in the prediction performance test as there were overlapping in some crime classes. However, we can see clustering in some variable combinations such as *crim* and *rad*, *rad* and *nox* and *crim* and *zn*. 

In conclusion, there seems to be some variable combinations but mainly *rad* as a single variable that can accurately separate different crime classes - particularly high and low crime.


